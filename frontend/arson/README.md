# "New Webpack" Build System

This is a radically experimental take on the Blackstorm build system as seen in Everwing. It is a learning project and nothing here is intended to be a firm direction for production.

The goal was to rebuild the build process from scratch using webpack and custom plugins rather than wrapping webpack with various tool. Doing this is a good way to make a more direct version of the build process that can be understood more easily.

In the process some features are sacrificed, although for the most part adding them back is easy (ie, adding appropriate build config for various release/debug build configs).

To build, run:
```
npm install
webpack
```

You can enable live dev by running:

```
webpack --watch
```

Currently you need to run your own web server from the appropriate subdirectory of `build/`.

## New Structure

The new structure is fairly straightforward:

1. Use stock webpack 3.
2. Config settings in webpack.config.js to match those generated by `jsio-webpack` and related config scripts.
3. A `BlackstormBuild` webpack plugin which performs game-specific custom steps (mostly asset processing and JSON generation).

We no longer use any JSIO or devkit code in the build process, exceptinging `devkit-spriter`.

Build specific logic/plugins are held in a `tools/` folder at the root of the repository. Generic webpack plugins are included as NPM packages.

## Advantages

In no particular order, there are a number of practical benefits from this approach:
1. Developers can understand 90% of build process from what is in `webpack.config.js` and the `tools/` folder. Only very specific features (like the spriting logic or other webpack plugins) require inspecting other locations. Developers only need to work through a thousand or so LOC in a central location instead of locating many files spread across a large set of repositories.
2. Because it's much easier to understand, developers will be more comfortable modifying the build process. Examples might include adding a step to copy a file required by Facebook or adding new build variants for other messengers/runtime environments.
3. There are no modifications/rewrites of configuration data through secondary APIs like `jsio-webpack` provides. This is a mixed bag as the webpack config will need to get more complex to support more build variants - so we may end up bringing some of this logic back. However, hopefully we can do it in such a way as to be more direct to understand and modify.
4. Due to the custom build plugin, caching, copying, and injecting data to the build can be implemented in a direct, consistent, and relatively easy to follow way.

## TODO

Make a self contained way to host the app so a secondary web server instance isn't required. Perhaps `--watch` could automatically enable this behavior, or we might continue with a separate `npm run serve` command.

Integrate the graphics compression tool directly into the build plugin (including using its caching system).

Implement the various build configurations (debug/release, different sets of credentials/flags, etc).

Some sprite sheets are not being grouped quite right. Fix metadata matching/rule logic.

Refactor the caching API to be easier to implement. Probably make build steps functions wrapped by caching system with a proxy class for disk IO so that asset processing steps don't determine caching, but rather they are black boxes managed by the cache system.

For slow steps (ie release mode graphics compression) we want to have a way to store the cached results. Perhaps best strategy is to upload cached items to an S3 (or Google equivalent) bucket so the team and build box can share them. I am a little nervous this could cause issues if developer systems diverge, but if we inject version numbers into our hashes we should be able to mitigate the problem. Additionally, it's most likely the case that it's better to match what a dev saw on their system than to change it at the final automated build step.

We have two cache levels - content based caching and then in watch mode a file size/mtime based system. The latter system uses `fs.stat` to detect changes. It turns out that `fs.stat` seems to be very slow in Node historically (as reported on SO). Benchmark to confirm this is still a problem, and consider adding a native node plugin or simply a secondary shell script or C program that can be run to accelerate the checks. We may also be able to piggy back on the webpack caching syste for this element.

Watch mode doesn't clearly report when errors occur - it always fires a "done" notification even if no results are emitted due to errors. Detect errors and clearly indicate to users this is the case.

Evaluate live reload (ie, event from webpack that triggers reload in browser) or hot reload (ie, push new code modules from webpack into running app).

Implement a sweet multi progress bar which shows asset build progress alongside standard webpack progress. Ideally it does not get messed up when log output is emitted.

Log output from webpack process to a `build.log` file.

Use a cluster task system to offload slower steps like graphics compression. Spriter may also be a target, but it may be better to implement a better packing algorithm and also disable most compression on spriter output (since the graphics compression tool will do a better and more configurable job even at "fast" settings).

Only run graphics compression tool when an image file is emitted or copied (probably in memory) not on the release folder every time.

## Learnings & Notes

Surprisingly, synchronous disk IO is much faster when on an SSD. Avoiding async IO and `Promise`s led to a major speedup in asset build process.

The build plugin approach dramatically simplifies custom build steps, although sometimes it is non-obvious what callbacks to use for certain functionalities. Webpack ecosystem is rich enough to make this manageable so far.

Multiple caching systems are hard to understand and maintain. Right now we have a spriter system, a webpack system, and probably one or two more. Moving spriting process to a single cache system made the behavior easier to follow, even though the implementation became a little more complex. The cache system API should be refactored to make caching easier to implement - ie treat work functions as black boxes that are wrapped by the cache system, instead of wiring cache checks directly into the work function logic.